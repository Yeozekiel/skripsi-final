{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34600986",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T20:03:28.210326Z",
     "iopub.status.busy": "2025-03-27T20:03:28.209949Z",
     "iopub.status.idle": "2025-03-27T20:03:33.081382Z",
     "shell.execute_reply": "2025-03-27T20:03:33.080471Z"
    },
    "papermill": {
     "duration": 4.924362,
     "end_time": "2025-03-27T20:03:33.082799",
     "exception": false,
     "start_time": "2025-03-27T20:03:28.158437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x14307390350>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import math\n",
    "import nltk\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import sacrebleu\n",
    "from torch.optim.lr_scheduler import _LRScheduler, CosineAnnealingLR, ReduceLROnPlateau\n",
    "import os\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction, corpus_bleu\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53555d0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T20:03:33.183324Z",
     "iopub.status.busy": "2025-03-27T20:03:33.182810Z",
     "iopub.status.idle": "2025-03-27T20:03:33.186719Z",
     "shell.execute_reply": "2025-03-27T20:03:33.186001Z"
    },
    "papermill": {
     "duration": 0.055741,
     "end_time": "2025-03-27T20:03:33.188042",
     "exception": false,
     "start_time": "2025-03-27T20:03:33.132301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATE = \"23Maret-NoKeyMask-embffn5122048Layer6H4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5277f537",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T20:03:33.289421Z",
     "iopub.status.busy": "2025-03-27T20:03:33.289083Z",
     "iopub.status.idle": "2025-03-27T20:03:33.293297Z",
     "shell.execute_reply": "2025-03-27T20:03:33.292623Z"
    },
    "papermill": {
     "duration": 0.05701,
     "end_time": "2025-03-27T20:03:33.294714",
     "exception": false,
     "start_time": "2025-03-27T20:03:33.237704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fungsi untuk membaca dataset JSON\n",
    "def load_dataset(json_file):\n",
    "    with open(f\"data2/{json_file}\", \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return [entry[\"text\"] for entry in data], [entry[\"label\"] for entry in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "988fefbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T20:03:33.399455Z",
     "iopub.status.busy": "2025-03-27T20:03:33.399066Z",
     "iopub.status.idle": "2025-03-27T20:03:33.664801Z",
     "shell.execute_reply": "2025-03-27T20:03:33.663736Z"
    },
    "papermill": {
     "duration": 0.320565,
     "end_time": "2025-03-27T20:03:33.666559",
     "exception": false,
     "start_time": "2025-03-27T20:03:33.345994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract text and labels\n",
    "src_texts, tgt_texts = load_dataset(\"train.json\")\n",
    "val_src_texts, val_tgt_texts = load_dataset(\"validation.json\")\n",
    "test_src_texts, test_tgt_texts = load_dataset(\"test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2792d77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T20:03:33.810724Z",
     "iopub.status.busy": "2025-03-27T20:03:33.810327Z",
     "iopub.status.idle": "2025-03-27T20:03:33.814235Z",
     "shell.execute_reply": "2025-03-27T20:03:33.813404Z"
    },
    "papermill": {
     "duration": 0.056831,
     "end_time": "2025-03-27T20:03:33.815868",
     "exception": false,
     "start_time": "2025-03-27T20:03:33.759037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenizers\n",
    "src_tokenizer = get_tokenizer(\"basic_english\")\n",
    "tgt_tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "204fdf1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T20:03:33.919701Z",
     "iopub.status.busy": "2025-03-27T20:03:33.919401Z",
     "iopub.status.idle": "2025-03-27T20:03:33.923796Z",
     "shell.execute_reply": "2025-03-27T20:03:33.923101Z"
    },
    "papermill": {
     "duration": 0.058367,
     "end_time": "2025-03-27T20:03:33.925078",
     "exception": false,
     "start_time": "2025-03-27T20:03:33.866711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Menghapus tanda baca dan mengonversi ke huruf kecil.\"\"\"\n",
    "    text = text.lower()  # Ubah ke huruf kecil\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)  # Hapus tanda baca\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Hapus spasi berlebih\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb221ec2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T20:03:34.026187Z",
     "iopub.status.busy": "2025-03-27T20:03:34.025901Z",
     "iopub.status.idle": "2025-03-27T20:03:35.106478Z",
     "shell.execute_reply": "2025-03-27T20:03:35.105539Z"
    },
    "papermill": {
     "duration": 1.132309,
     "end_time": "2025-03-27T20:03:35.108198",
     "exception": false,
     "start_time": "2025-03-27T20:03:33.975889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build vocabularies\n",
    "def tokenize_cleaned(text, tokenizer):\n",
    "    return tokenizer(text)\n",
    "\n",
    "def yield_tokens(texts, tokenizer):\n",
    "    for text in texts:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "src_vocab = build_vocab_from_iterator(yield_tokens(src_texts, src_tokenizer), specials=[\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"])\n",
    "tgt_vocab = build_vocab_from_iterator(yield_tokens(tgt_texts, tgt_tokenizer), specials=[\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"])\n",
    "\n",
    "src_vocab.set_default_index(src_vocab[\"<unk>\"])\n",
    "tgt_vocab.set_default_index(tgt_vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "914a0b64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T20:03:35.214908Z",
     "iopub.status.busy": "2025-03-27T20:03:35.214551Z",
     "iopub.status.idle": "2025-03-27T20:03:35.280279Z",
     "shell.execute_reply": "2025-03-27T20:03:35.279255Z"
    },
    "papermill": {
     "duration": 0.120277,
     "end_time": "2025-03-27T20:03:35.281881",
     "exception": false,
     "start_time": "2025-03-27T20:03:35.161604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pad_idx = src_vocab.get_stoi()[\"<pad>\"]\n",
    "sos_idx = src_vocab.get_stoi()[\"<sos>\"]\n",
    "eos_idx = src_vocab.get_stoi()[\"<eos>\"]\n",
    "unk_idx = src_vocab.get_stoi()[\"<unk>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92543888",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T20:03:35.390276Z",
     "iopub.status.busy": "2025-03-27T20:03:35.389938Z",
     "iopub.status.idle": "2025-03-27T20:03:35.394924Z",
     "shell.execute_reply": "2025-03-27T20:03:35.394072Z"
    },
    "papermill": {
     "duration": 0.060565,
     "end_time": "2025-03-27T20:03:35.396187",
     "exception": false,
     "start_time": "2025-03-27T20:03:35.335622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah token dalam vocabulary sumber (Javanese): 23876\n",
      "Jumlah token dalam vocabulary target (Indonesian): 15229\n"
     ]
    }
   ],
   "source": [
    "print(f\"Jumlah token dalam vocabulary sumber (Javanese): {len(src_vocab)}\")\n",
    "print(f\"Jumlah token dalam vocabulary target (Indonesian): {len(tgt_vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "534f1ee4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T20:03:35.499800Z",
     "iopub.status.busy": "2025-03-27T20:03:35.499465Z",
     "iopub.status.idle": "2025-03-27T20:03:35.504319Z",
     "shell.execute_reply": "2025-03-27T20:03:35.503613Z"
    },
    "papermill": {
     "duration": 0.057572,
     "end_time": "2025-03-27T20:03:35.505734",
     "exception": false,
     "start_time": "2025-03-27T20:03:35.448162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert text to tensor\n",
    "def text_to_tensor(text, vocab, tokenizer):\n",
    "    tokens = [vocab[\"<sos>\"]] + [vocab[token] for token in tokenizer(text)] + [vocab[\"<eos>\"]]\n",
    "    return torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "# Convert tensor to text\n",
    "def tensor_to_text(tensor, vocab):\n",
    "    special_tokens = {\"<sos>\", \"<eos>\", \"<pad>\"}  # Token yang ingin dihapus\n",
    "    tokens = vocab.lookup_tokens(tensor.tolist())  # Konversi tensor ke token\n",
    "    filtered_tokens = [token for token in tokens if token not in special_tokens]  # Hapus token spesial\n",
    "    return \" \".join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef01f809",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T20:03:35.607313Z",
     "iopub.status.busy": "2025-03-27T20:03:35.606979Z",
     "iopub.status.idle": "2025-03-27T20:03:35.613223Z",
     "shell.execute_reply": "2025-03-27T20:03:35.612403Z"
    },
    "papermill": {
     "duration": 0.05906,
     "end_time": "2025-03-27T20:03:35.614659",
     "exception": false,
     "start_time": "2025-03-27T20:03:35.555599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_texts, tgt_texts, src_vocab, tgt_vocab, src_tokenizer, tgt_tokenizer):\n",
    "        self.src_texts = [clean_text(text) for text in src_texts]\n",
    "        self.tgt_texts = [clean_text(text) for text in tgt_texts]\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_tensor = text_to_tensor(self.src_texts[idx], self.src_vocab, self.src_tokenizer)\n",
    "        tgt_tensor = text_to_tensor(self.tgt_texts[idx], self.tgt_vocab, self.tgt_tokenizer)\n",
    "        return src_tensor, tgt_tensor\n",
    "\n",
    "# DataLoader with padding\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_batch = pad_sequence(src_batch, padding_value=src_vocab[\"<pad>\"], batch_first=True)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=tgt_vocab[\"<pad>\"], batch_first=True)\n",
    "    return src_batch, tgt_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba22e658",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T20:03:35.717454Z",
     "iopub.status.busy": "2025-03-27T20:03:35.717082Z",
     "iopub.status.idle": "2025-03-27T20:03:35.720909Z",
     "shell.execute_reply": "2025-03-27T20:03:35.719942Z"
    },
    "papermill": {
     "duration": 0.056226,
     "end_time": "2025-03-27T20:03:35.722408",
     "exception": false,
     "start_time": "2025-03-27T20:03:35.666182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91327a80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T20:03:35.824152Z",
     "iopub.status.busy": "2025-03-27T20:03:35.823852Z",
     "iopub.status.idle": "2025-03-27T20:03:36.750429Z",
     "shell.execute_reply": "2025-03-27T20:03:36.749426Z"
    },
    "papermill": {
     "duration": 0.980056,
     "end_time": "2025-03-27T20:03:36.752713",
     "exception": false,
     "start_time": "2025-03-27T20:03:35.772657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = TranslationDataset(src_texts, tgt_texts, src_vocab, tgt_vocab, src_tokenizer, tgt_tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "val_dataset = TranslationDataset(val_src_texts, val_tgt_texts, src_vocab, tgt_vocab, src_tokenizer, tgt_tokenizer)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_dataset = TranslationDataset(test_src_texts, test_tgt_texts, src_vocab, tgt_vocab, src_tokenizer, tgt_tokenizer)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "680775db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T20:03:36.876390Z",
     "iopub.status.busy": "2025-03-27T20:03:36.876028Z",
     "iopub.status.idle": "2025-03-27T20:03:36.892850Z",
     "shell.execute_reply": "2025-03-27T20:03:36.891769Z"
    },
    "papermill": {
     "duration": 0.081667,
     "end_time": "2025-03-27T20:03:36.894614",
     "exception": false,
     "start_time": "2025-03-27T20:03:36.812947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 60]) torch.Size([64, 61])\n",
      "sampun ngantos namanipun yonatan kabusek saking tedhak turunipun dhimas dawud ugi manawi pangeran yehuwah badhe males ukum dhateng mengsah mengsahipun dhimas dawud\n",
      "janganlah nama yonatan terhapus dari keturunan daud melainkan kiranya tuhan menuntut balas dari pada musuh musuh daud\n"
     ]
    }
   ],
   "source": [
    "for src_batch, tgt_batch in dataloader:\n",
    "    print(src_batch.shape, tgt_batch.shape)\n",
    "    print(tensor_to_text(src_batch[0], src_vocab))\n",
    "    print(tensor_to_text(tgt_batch[0], tgt_vocab))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25e4f8d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T20:03:37.011042Z",
     "iopub.status.busy": "2025-03-27T20:03:37.010709Z",
     "iopub.status.idle": "2025-03-27T20:03:37.016973Z",
     "shell.execute_reply": "2025-03-27T20:03:37.015971Z"
    },
    "papermill": {
     "duration": 0.070334,
     "end_time": "2025-03-27T20:03:37.018599",
     "exception": false,
     "start_time": "2025-03-27T20:03:36.948265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, : x.size(1), :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fdcd8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T20:03:37.140800Z",
     "iopub.status.busy": "2025-03-27T20:03:37.140408Z",
     "iopub.status.idle": "2025-03-27T20:03:37.154699Z",
     "shell.execute_reply": "2025-03-27T20:03:37.153628Z"
    },
    "papermill": {
     "duration": 0.082099,
     "end_time": "2025-03-27T20:03:37.156560",
     "exception": false,
     "start_time": "2025-03-27T20:03:37.074461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerMT(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_size=256, num_heads=8, num_layers=4, ff_dim=512, max_len=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, embed_size)\n",
    "        self.pos_encoding = PositionalEncoding(embed_size, max_len)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=embed_size, nhead=num_heads, dim_feedforward=ff_dim, dropout=dropout, batch_first=True), \n",
    "            num_layers=num_layers,\n",
    "            norm = nn.LayerNorm(embed_size)\n",
    "        )\n",
    "\n",
    "        # self.enc_norm = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model=embed_size, nhead=num_heads, dim_feedforward=ff_dim, dropout=dropout, batch_first=True),\n",
    "            num_layers=num_layers,\n",
    "            norm = nn.LayerNorm(embed_size)\n",
    "        )\n",
    "\n",
    "        # self.dec_norm = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_size, tgt_vocab_size)\n",
    "        self.src_pad_idx = src_vocab[\"<pad>\"]\n",
    "        self.tgt_pad_idx = tgt_vocab[\"<pad>\"]\n",
    "\n",
    "    def generate_key_padding_mask(self, seq, pad_idx):\n",
    "        mask = (seq == pad_idx).float()  # Mask posisi padding\n",
    "        return mask.masked_fill(mask == 1, float('-inf')).masked_fill(mask == 0, float(0.0))\n",
    "\n",
    "    def generate_subsequent_mask(self, size):\n",
    "        mask = torch.triu(torch.ones(size, size), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, float('-inf')).masked_fill(mask == 0, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, tgt, return_encoder=False, return_attention=False):\n",
    "        src_emb = self.dropout(self.pos_encoding(self.src_embedding(src)))\n",
    "        tgt_emb = self.dropout(self.pos_encoding(self.tgt_embedding(tgt)))\n",
    "\n",
    "        src_key_padding_mask = None #self.generate_key_padding_mask(src, self.src_pad_idx).to(src.device)\n",
    "        tgt_key_padding_mask = None #self.generate_key_padding_mask(tgt, self.tgt_pad_idx).to(tgt.device)\n",
    "\n",
    "\n",
    "        tgt_mask = self.generate_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
    "\n",
    "        enc_output = self.encoder(src_emb, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        dec_output = self.decoder(\n",
    "            tgt_emb, enc_output, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask, \n",
    "            memory_key_padding_mask=src_key_padding_mask\n",
    "        )\n",
    "        if return_attention:\n",
    "            attn_scores = self.decoder.layers[0].multihead_attn.attn_output_weights\n",
    "            return self.fc_out(dec_output), attn_scores\n",
    "\n",
    "        if return_encoder:\n",
    "            return self.fc_out(dec_output), enc_output\n",
    "        else:\n",
    "            return self.fc_out(dec_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "552eb180",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T20:03:37.512101Z",
     "iopub.status.busy": "2025-03-27T20:03:37.511779Z",
     "iopub.status.idle": "2025-03-27T20:03:37.517078Z",
     "shell.execute_reply": "2025-03-27T20:03:37.516160Z"
    },
    "papermill": {
     "duration": 0.060688,
     "end_time": "2025-03-27T20:03:37.518766",
     "exception": false,
     "start_time": "2025-03-27T20:03:37.458078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerLRScheduler:\n",
    "    def __init__(self, d_model=512, warmup_steps=4000):\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.current_step = 0\n",
    "\n",
    "    def step(self, optimizer):\n",
    "        \"\"\"Update learning rate based on current step.\"\"\"\n",
    "        self.current_step += 1\n",
    "        lr = self.compute_lr()\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def compute_lr(self):\n",
    "        \"\"\"Compute learning rate at current step.\"\"\"\n",
    "        scale = self.d_model ** -0.5\n",
    "        step_factor = min(self.current_step ** -0.5, self.current_step * self.warmup_steps ** -1.5)\n",
    "        return scale * step_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "138f581a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T20:03:37.628140Z",
     "iopub.status.busy": "2025-03-27T20:03:37.627704Z",
     "iopub.status.idle": "2025-03-27T20:03:37.632746Z",
     "shell.execute_reply": "2025-03-27T20:03:37.631828Z"
    },
    "papermill": {
     "duration": 0.062363,
     "end_time": "2025-03-27T20:03:37.634321",
     "exception": false,
     "start_time": "2025-03-27T20:03:37.571958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embed_size=512\n",
    "num_heads=4\n",
    "num_layers=6\n",
    "ff_dim=2048\n",
    "max_len=128\n",
    "dropout = 0.3\n",
    "warmup_steps=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ddb8caae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T20:03:37.745981Z",
     "iopub.status.busy": "2025-03-27T20:03:37.745631Z",
     "iopub.status.idle": "2025-03-27T20:03:41.589669Z",
     "shell.execute_reply": "2025-03-27T20:03:41.588849Z"
    },
    "papermill": {
     "duration": 3.901638,
     "end_time": "2025-03-27T20:03:41.591446",
     "exception": false,
     "start_time": "2025-03-27T20:03:37.689808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerMT(len(src_vocab), len(tgt_vocab), embed_size, num_heads, num_layers, ff_dim, max_len, dropout)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001) #lr=0.001\n",
    "scheduler = TransformerLRScheduler(d_model=embed_size)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_bleu_score = float(\"-inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f3d146a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T20:03:41.906070Z",
     "iopub.status.busy": "2025-03-27T20:03:41.905550Z",
     "iopub.status.idle": "2025-03-27T20:03:41.915327Z",
     "shell.execute_reply": "2025-03-27T20:03:41.914590Z"
    },
    "papermill": {
     "duration": 0.064549,
     "end_time": "2025-03-27T20:03:41.916665",
     "exception": false,
     "start_time": "2025-03-27T20:03:41.852116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_bleu(model, dataloader, device, src_vocab, tgt_vocab, max_len=64):\n",
    "    model.eval()\n",
    "    hypotheses = []\n",
    "    references = []\n",
    "\n",
    "    with tqdm(total=len(dataloader), desc=\"Evaluating BLEU\") as pbar:\n",
    "        for src, tgt in dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "            for i in range(src.size(0)):\n",
    "                src_input = src[i].unsqueeze(0)  # Tambahkan batch dimensi\n",
    "                enc_output = model.encoder(model.dropout(model.pos_encoding(model.src_embedding(src_input))))\n",
    "\n",
    "                tgt_tokens = [tgt_vocab[\"<sos>\"]]  # Mulai dengan token <sos>\n",
    "\n",
    "                for _ in range(max_len):\n",
    "                    tgt_tensor = torch.tensor(tgt_tokens, dtype=torch.long, device=device).unsqueeze(0)  # (1, len)\n",
    "                    tgt_emb = model.dropout(model.pos_encoding(model.tgt_embedding(tgt_tensor)))\n",
    "\n",
    "                    tgt_mask = model.generate_subsequent_mask(tgt_tensor.size(1)).to(device)\n",
    "\n",
    "                    dec_output = model.decoder(tgt_emb, enc_output, tgt_mask=tgt_mask)\n",
    "                    next_token_logits = model.fc_out(dec_output[:, -1, :])  # Ambil token terakhir\n",
    "                    next_token = next_token_logits.argmax(dim=-1).item()\n",
    "\n",
    "                    if next_token == tgt_vocab[\"<eos>\"]:  # Stop jika mencapai <eos>\n",
    "                        break\n",
    "\n",
    "                    tgt_tokens.append(next_token)\n",
    "\n",
    "                pred_text = tensor_to_text(torch.tensor(tgt_tokens).detach().cpu(), tgt_vocab)  # Detach & pindahkan ke CPU\n",
    "                ref_text = tensor_to_text(tgt[i].detach().cpu(), tgt_vocab)  # Pindahkan ke CPU\n",
    "\n",
    "                hypotheses.append(pred_text)\n",
    "                references.append(ref_text)\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Hitung BLEU score dengan sacreBLEU\n",
    "    bleu_score = sacrebleu.corpus_bleu(hypotheses, [references]).score\n",
    "    return bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1caa44d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T20:03:42.020168Z",
     "iopub.status.busy": "2025-03-27T20:03:42.019824Z",
     "iopub.status.idle": "2025-03-27T20:03:42.031803Z",
     "shell.execute_reply": "2025-03-27T20:03:42.030756Z"
    },
    "papermill": {
     "duration": 0.065295,
     "end_time": "2025-03-27T20:03:42.033269",
     "exception": false,
     "start_time": "2025-03-27T20:03:41.967974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for src, tgt in dataloader:\n",
    "    source = src.to(device)\n",
    "    targer = tgt.to(device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66efa182",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T20:03:42.143262Z",
     "iopub.status.busy": "2025-03-27T20:03:42.142924Z",
     "iopub.status.idle": "2025-03-27T20:03:42.146238Z",
     "shell.execute_reply": "2025-03-27T20:03:42.145459Z"
    },
    "papermill": {
     "duration": 0.059856,
     "end_time": "2025-03-27T20:03:42.147747",
     "exception": false,
     "start_time": "2025-03-27T20:03:42.087891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from torchview import draw_graph\n",
    "\n",
    "# model_graph = draw_graph(model, input_data=(src, tgt), expand_nested=True)\n",
    "# model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f69b9f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T20:03:42.257675Z",
     "iopub.status.busy": "2025-03-27T20:03:42.257264Z",
     "iopub.status.idle": "2025-03-27T20:03:42.264171Z",
     "shell.execute_reply": "2025-03-27T20:03:42.263246Z"
    },
    "papermill": {
     "duration": 0.063977,
     "end_time": "2025-03-27T20:03:42.265662",
     "exception": false,
     "start_time": "2025-03-27T20:03:42.201685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters    : 71974781\n",
      "Trained Parameters  : 71974781\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trained_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trained_params\n",
    "\n",
    "total_params, trained_params = count_parameters(model)\n",
    "\n",
    "print(f\"Total Parameters    : {total_params}\")\n",
    "print(f\"Trained Parameters  : {trained_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "546e8f1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T20:03:42.373099Z",
     "iopub.status.busy": "2025-03-27T20:03:42.372783Z",
     "iopub.status.idle": "2025-03-27T20:03:42.379780Z",
     "shell.execute_reply": "2025-03-27T20:03:42.378969Z"
    },
    "papermill": {
     "duration": 0.062262,
     "end_time": "2025-03-27T20:03:42.381166",
     "exception": false,
     "start_time": "2025-03-27T20:03:42.318904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def greedy_decode(model, src, src_vocab, tgt_vocab, device, max_len=64):\n",
    "    model.eval()\n",
    "    src = src.unsqueeze(0).to(device)  # Tambahkan batch dimensi\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc_output = model.enc_norm(model.encoder(model.dropout(model.pos_encoding(model.src_embedding(src)))))\n",
    "\n",
    "    tgt_tokens = [tgt_vocab[\"<sos>\"]]  # Mulai dengan token <sos>\n",
    "\n",
    "    with tqdm(total=max_len, desc=\"Decoding\", leave=False) as pbar:\n",
    "        for _ in range(max_len):\n",
    "            tgt_tensor = torch.tensor(tgt_tokens, dtype=torch.long, device=device).unsqueeze(0)  # (1, len)\n",
    "            tgt_emb = model.dropout(model.pos_encoding(model.tgt_embedding(tgt_tensor)))\n",
    "\n",
    "            tgt_mask = model.generate_subsequent_mask(tgt_tensor.size(1)).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                dec_output = model.dec_norm(model.decoder(tgt_emb, enc_output, tgt_mask=tgt_mask))\n",
    "                next_token_logits = model.fc_out(dec_output[:, -1, :])  # Ambil token terakhir\n",
    "                next_token = next_token_logits.argmax(dim=-1).item()\n",
    "\n",
    "            if next_token == tgt_vocab[\"<eos>\"]:  # Stop jika mencapai <eos>\n",
    "                break\n",
    "\n",
    "            tgt_tokens.append(next_token)\n",
    "            pbar.update(1)  # Update progress bar setiap iterasi\n",
    "\n",
    "    return tensor_to_text(torch.tensor(tgt_tokens, dtype=torch.long, device=device), tgt_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29245105",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T20:03:42.485366Z",
     "iopub.status.busy": "2025-03-27T20:03:42.484967Z",
     "iopub.status.idle": "2025-03-27T20:03:42.494390Z",
     "shell.execute_reply": "2025-03-27T20:03:42.493572Z"
    },
    "papermill": {
     "duration": 0.062999,
     "end_time": "2025-03-27T20:03:42.495868",
     "exception": false,
     "start_time": "2025-03-27T20:03:42.432869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def beam_search_decode(model, src, src_vocab=src_vocab, tgt_vocab=tgt_vocab, device=device, beam_size=5, max_len=64, temperature=1.0):\n",
    "    model.eval()\n",
    "    src = src.unsqueeze(0).to(device)  # Tambahkan batch dimensi\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc_output = model.enc_norm(model.encoder(model.dropout(model.pos_encoding(model.src_embedding(src)))))\n",
    "\n",
    "    # Inisialisasi beam search dengan (score, token sequence)\n",
    "    beams = [(0, [tgt_vocab[\"<sos>\"]])]  # Log probabilitas awal = 0\n",
    "\n",
    "    with tqdm(total=max_len, desc=\"Beam Decoding\", leave=False) as pbar:\n",
    "        for _ in range(max_len):\n",
    "            all_candidates = []\n",
    "\n",
    "            for score, tgt_tokens in beams:\n",
    "                tgt_tensor = torch.tensor(tgt_tokens, dtype=torch.long, device=device).unsqueeze(0)  # (1, len)\n",
    "                tgt_emb = model.dropout(model.pos_encoding(model.tgt_embedding(tgt_tensor)))\n",
    "\n",
    "                tgt_mask = model.generate_subsequent_mask(tgt_tensor.size(1)).to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    dec_output = model.dec_norm(model.decoder(tgt_emb, enc_output, tgt_mask=tgt_mask))\n",
    "                    logits = model.fc_out(dec_output[:, -1, :])  # Ambil token terakhir\n",
    "\n",
    "                # Terapkan temperature scaling\n",
    "                logits = logits / temperature\n",
    "                probs = F.softmax(logits, dim=-1)  # Konversi ke probabilitas\n",
    "                log_probs = torch.log(probs + 1e-9)  # Hindari log(0)\n",
    "\n",
    "                # Ambil top-k kandidat\n",
    "                top_log_probs, top_indices = log_probs.topk(beam_size)\n",
    "\n",
    "                for i in range(beam_size):\n",
    "                    next_token = top_indices[0, i].item()\n",
    "                    new_score = score + top_log_probs[0, i].item()  # Akumulasi log probabilitas\n",
    "                    new_sequence = tgt_tokens + [next_token]\n",
    "                    all_candidates.append((new_score, new_sequence))\n",
    "\n",
    "            # Pilih `beam_size` terbaik berdasarkan skor tertinggi\n",
    "            beams = sorted(all_candidates, key=lambda x: x[0], reverse=True)[:beam_size]\n",
    "\n",
    "            # Cek apakah semua beam sudah mencapai <eos>\n",
    "            if all(next_token == tgt_vocab[\"<eos>\"] for _, seq in beams for next_token in [seq[-1]]):\n",
    "                break\n",
    "\n",
    "            pbar.update(1)  # Update progress bar setiap iterasi\n",
    "\n",
    "    # Ambil sequence dengan skor tertinggi\n",
    "    best_sequence = max(beams, key=lambda x: x[0])[1]\n",
    "\n",
    "    return tensor_to_text(torch.tensor(best_sequence, dtype=torch.long, device=device), tgt_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b3b912",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T20:03:42.601393Z",
     "iopub.status.busy": "2025-03-27T20:03:42.601045Z",
     "iopub.status.idle": "2025-03-27T20:03:42.611061Z",
     "shell.execute_reply": "2025-03-27T20:03:42.610305Z"
    },
    "papermill": {
     "duration": 0.064343,
     "end_time": "2025-03-27T20:03:42.612386",
     "exception": false,
     "start_time": "2025-03-27T20:03:42.548043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def beam_search_decode_text(model, src_text, src_vocab, tgt_vocab, device, beam_size=5, max_len=64, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Beam Search Decoding untuk input teks.\n",
    "\n",
    "    Args:\n",
    "        model: Model Transformer yang telah dilatih.\n",
    "        src_text (str): Kalimat input dalam bentuk teks.\n",
    "        src_vocab (dict): Kamus subword ke indeks untuk bahasa sumber.\n",
    "        tgt_vocab (dict): Kamus subword ke indeks untuk bahasa target.\n",
    "        device: Perangkat (CPU/GPU).\n",
    "        beam_size (int): Jumlah kandidat yang dijaga selama decoding.\n",
    "        max_len (int): Panjang maksimum output terjemahan.\n",
    "        temperature (float): Faktor untuk scaling logits (default=1.0).\n",
    "\n",
    "    Returns:\n",
    "        str: Hasil terjemahan dalam bentuk teks.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Konversi teks sumber menjadi tensor indeks\n",
    "    src_tokens = text_to_tensor(src_text, src_vocab).to(device).unsqueeze(0)  # Tambahkan batch dimensi\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc_output = model.enc_norm(model.encoder(model.dropout(model.pos_encoding(model.src_embedding(src_tokens)))))\n",
    "\n",
    "    # Inisialisasi beam search dengan (score, token sequence)\n",
    "    beams = [(0, [tgt_vocab[\"<sos>\"]])]  # Log probabilitas awal = 0\n",
    "\n",
    "    with tqdm(total=max_len, desc=\"Beam Decoding\", leave=False) as pbar:\n",
    "        for _ in range(max_len):\n",
    "            all_candidates = []\n",
    "\n",
    "            for score, tgt_tokens in beams:\n",
    "                tgt_tensor = torch.tensor(tgt_tokens, dtype=torch.long, device=device).unsqueeze(0)  # (1, len)\n",
    "                tgt_emb = model.dropout(model.pos_encoding(model.tgt_embedding(tgt_tensor)))\n",
    "\n",
    "                tgt_mask = model.generate_subsequent_mask(tgt_tensor.size(1)).to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    dec_output = model.dec_norm(model.decoder(tgt_emb, enc_output, tgt_mask=tgt_mask))\n",
    "                    logits = model.fc_out(dec_output[:, -1, :])  # Ambil token terakhir\n",
    "\n",
    "                # Terapkan temperature scaling\n",
    "                logits = logits / temperature\n",
    "                probs = F.softmax(logits, dim=-1)  # Konversi ke probabilitas\n",
    "                log_probs = torch.log(probs + 1e-9)  # Hindari log(0)\n",
    "\n",
    "                # Ambil top-k kandidat\n",
    "                top_log_probs, top_indices = log_probs.topk(beam_size)\n",
    "\n",
    "                for i in range(beam_size):\n",
    "                    next_token = top_indices[0, i].item()\n",
    "                    new_score = score + top_log_probs[0, i].item()  # Akumulasi log probabilitas\n",
    "                    new_sequence = tgt_tokens + [next_token]\n",
    "                    all_candidates.append((new_score, new_sequence))\n",
    "\n",
    "            # Pilih `beam_size` terbaik berdasarkan skor tertinggi\n",
    "            beams = sorted(all_candidates, key=lambda x: x[0], reverse=True)[:beam_size]\n",
    "\n",
    "            # Cek apakah semua beam sudah mencapai <eos>\n",
    "            if all(next_token == tgt_vocab[\"<eos>\"] for _, seq in beams for next_token in [seq[-1]]):\n",
    "                break\n",
    "\n",
    "            pbar.update(1)  # Update progress bar setiap iterasi\n",
    "\n",
    "    # Ambil sequence dengan skor tertinggi\n",
    "    best_sequence = max(beams, key=lambda x: x[0])[1]\n",
    "\n",
    "    return tensor_to_text(torch.tensor(best_sequence, dtype=torch.long, device=device), tgt_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "69643783",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T20:03:42.720940Z",
     "iopub.status.busy": "2025-03-27T20:03:42.720594Z",
     "iopub.status.idle": "2025-03-27T20:03:42.731609Z",
     "shell.execute_reply": "2025-03-27T20:03:42.730792Z"
    },
    "papermill": {
     "duration": 0.066482,
     "end_time": "2025-03-27T20:03:42.733101",
     "exception": false,
     "start_time": "2025-03-27T20:03:42.666619",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, val_dataloader, optimizer, scheduler, num_epochs, device, src_vocab, tgt_vocab, save_dir=\"models\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)  # Pastikan folder penyimpanan ada\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    bleu_scores = []\n",
    "    learning_rates = []  # Menyimpan learning rate\n",
    "    best_bleu = 0  # BLEU score terbaik\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for src, tgt in progress_bar:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "            tgt_input = tgt[:, :-1]  # Input untuk decoder\n",
    "            tgt_output = tgt[:, 1:]  # Target untuk loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(src, tgt_input)\n",
    "            logits = logits.reshape(-1, logits.shape[-1])\n",
    "            tgt_output = tgt_output.reshape(-1)\n",
    "\n",
    "            loss = F.cross_entropy(logits, tgt_output, ignore_index=0, label_smoothing=0.1)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update learning rate\n",
    "            if scheduler is not None:\n",
    "                scheduler.step(optimizer)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(dataloader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # === Evaluasi (Validasi Loss) ===\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for src, tgt in val_dataloader:\n",
    "                src, tgt = src.to(device), tgt.to(device)\n",
    "                tgt_input = tgt[:, :-1]\n",
    "                tgt_output = tgt[:, 1:]\n",
    "\n",
    "                logits = model(src, tgt_input)\n",
    "                loss = F.cross_entropy(logits.reshape(-1, logits.shape[-1]), tgt_output.reshape(-1), ignore_index=0)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # === Evaluasi BLEU Setiap 20 Epoch ===\n",
    "        bleu_score = None\n",
    "        if (epoch + 1) % 20 == 0:  \n",
    "            bleu_score = evaluate_bleu(model, val_dataloader, device, src_vocab, tgt_vocab)\n",
    "            bleu_scores.append(bleu_score)\n",
    "            print(f\"\\n[Epoch {epoch+1}] BLEU: {bleu_score:.2f}\")\n",
    "\n",
    "            # Simpan model terbaik berdasarkan BLEU\n",
    "            if bleu_score > best_bleu:\n",
    "                best_bleu = bleu_score\n",
    "                torch.save(model.state_dict(), os.path.join(save_dir, f\"{DATE}_best_model.pth\"))\n",
    "                print(f\"🔥 Model terbaik disimpan dengan BLEU: {bleu_score:.2f}\")\n",
    "\n",
    "        # Simpan learning rate\n",
    "        if scheduler is not None:\n",
    "            lr = optimizer.param_groups[0]['lr']  # Ambil learning rate terbaru\n",
    "            learning_rates.append(lr)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}, \")\n",
    "        print(f\"Learning Rate: {learning_rates[-1]:.6f}\")\n",
    "\n",
    "        # Simpan model setiap 10 epoch\n",
    "        if (epoch + 1) % 60 == 0:\n",
    "            torch.save(model.state_dict(), os.path.join(save_dir, f\"{DATE}_model_epoch_{epoch+1}.pth\"))\n",
    "            print(f\"✅ Model disimpan: {DATE}_model_epoch_{epoch+1}.pth\")\n",
    "\n",
    "    return train_losses, val_losses, bleu_scores, learning_rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "357d26af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T20:03:42.948173Z",
     "iopub.status.busy": "2025-03-27T20:03:42.947867Z",
     "iopub.status.idle": "2025-03-27T23:42:14.202307Z",
     "shell.execute_reply": "2025-03-27T23:42:14.201435Z"
    },
    "papermill": {
     "duration": 13111.311282,
     "end_time": "2025-03-27T23:42:14.203834",
     "exception": false,
     "start_time": "2025-03-27T20:03:42.892552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/120: 100%|██████████| 364/364 [01:23<00:00,  4.35it/s, loss=6.8441]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: Train Loss = 7.9032, Val Loss = 6.5143, \n",
      "Learning Rate: 0.000064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/120: 100%|██████████| 364/364 [01:23<00:00,  4.36it/s, loss=6.1901]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: Train Loss = 6.6574, Val Loss = 5.7581, \n",
      "Learning Rate: 0.000127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/120: 100%|██████████| 364/364 [01:23<00:00,  4.37it/s, loss=5.5475]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: Train Loss = 6.0667, Val Loss = 5.1495, \n",
      "Learning Rate: 0.000191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/120: 100%|██████████| 364/364 [01:23<00:00,  4.37it/s, loss=4.9685]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: Train Loss = 5.5981, Val Loss = 4.6833, \n",
      "Learning Rate: 0.000254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/120: 100%|██████████| 364/364 [01:23<00:00,  4.37it/s, loss=4.4867]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: Train Loss = 5.2258, Val Loss = 4.3104, \n",
      "Learning Rate: 0.000318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/120: 100%|██████████| 364/364 [01:23<00:00,  4.37it/s, loss=3.9264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6: Train Loss = 4.9220, Val Loss = 4.0266, \n",
      "Learning Rate: 0.000382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/120: 100%|██████████| 364/364 [01:23<00:00,  4.37it/s, loss=3.6419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7: Train Loss = 4.6679, Val Loss = 3.8351, \n",
      "Learning Rate: 0.000445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/120: 100%|██████████| 364/364 [01:23<00:00,  4.37it/s, loss=3.3265]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: Train Loss = 4.4568, Val Loss = 3.6657, \n",
      "Learning Rate: 0.000509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/120: 100%|██████████| 364/364 [01:23<00:00,  4.37it/s, loss=3.0245]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9: Train Loss = 4.2775, Val Loss = 3.5595, \n",
      "Learning Rate: 0.000572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/120: 100%|██████████| 364/364 [01:23<00:00,  4.37it/s, loss=2.8784]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: Train Loss = 4.1387, Val Loss = 3.4521, \n",
      "Learning Rate: 0.000636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/120: 100%|██████████| 364/364 [01:23<00:00,  4.37it/s, loss=2.7471]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11: Train Loss = 4.0255, Val Loss = 3.3587, \n",
      "Learning Rate: 0.000698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/120: 100%|██████████| 364/364 [01:23<00:00,  4.37it/s, loss=2.6636]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12: Train Loss = 3.9122, Val Loss = 3.2817, \n",
      "Learning Rate: 0.000669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/120: 100%|██████████| 364/364 [01:23<00:00,  4.37it/s, loss=2.5312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13: Train Loss = 3.7618, Val Loss = 3.1377, \n",
      "Learning Rate: 0.000642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/120: 100%|██████████| 364/364 [01:23<00:00,  4.37it/s, loss=2.3476]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14: Train Loss = 3.6248, Val Loss = 3.0483, \n",
      "Learning Rate: 0.000619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/120: 100%|██████████| 364/364 [01:23<00:00,  4.36it/s, loss=2.3512]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15: Train Loss = 3.4943, Val Loss = 2.9870, \n",
      "Learning Rate: 0.000598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/120: 100%|██████████| 364/364 [01:23<00:00,  4.37it/s, loss=2.1370]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16: Train Loss = 3.3838, Val Loss = 2.9266, \n",
      "Learning Rate: 0.000579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/120: 100%|██████████| 364/364 [01:23<00:00,  4.36it/s, loss=2.0803]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17: Train Loss = 3.2844, Val Loss = 2.8818, \n",
      "Learning Rate: 0.000562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/120: 100%|██████████| 364/364 [01:23<00:00,  4.37it/s, loss=2.0840]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18: Train Loss = 3.1994, Val Loss = 2.8378, \n",
      "Learning Rate: 0.000546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/120: 100%|██████████| 364/364 [01:23<00:00,  4.37it/s, loss=1.9765]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19: Train Loss = 3.1212, Val Loss = 2.8272, \n",
      "Learning Rate: 0.000531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/120: 100%|██████████| 364/364 [01:23<00:00,  4.38it/s, loss=1.9459]\n",
      "Evaluating BLEU: 100%|██████████| 49/49 [06:59<00:00,  8.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 20] BLEU: 21.34\n",
      "🔥 Model terbaik disimpan dengan BLEU: 21.34\n",
      "\n",
      "Epoch 20: Train Loss = 3.0528, Val Loss = 2.7840, \n",
      "Learning Rate: 0.000518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/120: 100%|██████████| 364/364 [01:23<00:00,  4.38it/s, loss=1.8666]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21: Train Loss = 2.9871, Val Loss = 2.7609, \n",
      "Learning Rate: 0.000505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/120: 100%|██████████| 364/364 [01:23<00:00,  4.38it/s, loss=1.8594]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22: Train Loss = 2.9247, Val Loss = 2.7371, \n",
      "Learning Rate: 0.000494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/120: 100%|██████████| 364/364 [01:23<00:00,  4.38it/s, loss=1.8885]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23: Train Loss = 2.8691, Val Loss = 2.7196, \n",
      "Learning Rate: 0.000483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/120: 100%|██████████| 364/364 [01:23<00:00,  4.38it/s, loss=1.9287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24: Train Loss = 2.8186, Val Loss = 2.7122, \n",
      "Learning Rate: 0.000473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/120: 100%|██████████| 364/364 [01:23<00:00,  4.38it/s, loss=1.8309]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25: Train Loss = 2.7725, Val Loss = 2.6890, \n",
      "Learning Rate: 0.000463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/120: 100%|██████████| 364/364 [01:23<00:00,  4.38it/s, loss=1.7704]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26: Train Loss = 2.7231, Val Loss = 2.6941, \n",
      "Learning Rate: 0.000454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/120: 100%|██████████| 364/364 [01:22<00:00,  4.39it/s, loss=1.8424]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27: Train Loss = 2.6816, Val Loss = 2.6933, \n",
      "Learning Rate: 0.000446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/120: 100%|██████████| 364/364 [01:23<00:00,  4.38it/s, loss=1.7739]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28: Train Loss = 2.6440, Val Loss = 2.6774, \n",
      "Learning Rate: 0.000438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/120: 100%|██████████| 364/364 [01:23<00:00,  4.38it/s, loss=1.7418]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29: Train Loss = 2.6034, Val Loss = 2.6682, \n",
      "Learning Rate: 0.000430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/120: 100%|██████████| 364/364 [01:23<00:00,  4.38it/s, loss=1.8013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30: Train Loss = 2.5694, Val Loss = 2.6603, \n",
      "Learning Rate: 0.000423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/120: 100%|██████████| 364/364 [01:23<00:00,  4.38it/s, loss=1.7233]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31: Train Loss = 2.5395, Val Loss = 2.6653, \n",
      "Learning Rate: 0.000416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/120: 100%|██████████| 364/364 [01:22<00:00,  4.39it/s, loss=1.7290]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32: Train Loss = 2.5062, Val Loss = 2.6525, \n",
      "Learning Rate: 0.000409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/120: 100%|██████████| 364/364 [01:22<00:00,  4.39it/s, loss=1.7338]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33: Train Loss = 2.4756, Val Loss = 2.6462, \n",
      "Learning Rate: 0.000403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/120: 100%|██████████| 364/364 [01:23<00:00,  4.38it/s, loss=1.7389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34: Train Loss = 2.4457, Val Loss = 2.6451, \n",
      "Learning Rate: 0.000397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/120: 100%|██████████| 364/364 [01:23<00:00,  4.38it/s, loss=1.7415]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35: Train Loss = 2.4221, Val Loss = 2.6441, \n",
      "Learning Rate: 0.000392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/120: 100%|██████████| 364/364 [01:22<00:00,  4.39it/s, loss=1.7261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36: Train Loss = 2.3932, Val Loss = 2.6529, \n",
      "Learning Rate: 0.000386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/120: 100%|██████████| 364/364 [01:23<00:00,  4.38it/s, loss=1.6883]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37: Train Loss = 2.3705, Val Loss = 2.6532, \n",
      "Learning Rate: 0.000381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/120: 100%|██████████| 364/364 [01:23<00:00,  4.38it/s, loss=1.7454]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38: Train Loss = 2.3457, Val Loss = 2.6642, \n",
      "Learning Rate: 0.000376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/120: 100%|██████████| 364/364 [01:23<00:00,  4.39it/s, loss=1.7354]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39: Train Loss = 2.3241, Val Loss = 2.6647, \n",
      "Learning Rate: 0.000371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/120: 100%|██████████| 364/364 [01:23<00:00,  4.38it/s, loss=1.6843]\n",
      "Evaluating BLEU: 100%|██████████| 49/49 [07:09<00:00,  8.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 40] BLEU: 26.28\n",
      "🔥 Model terbaik disimpan dengan BLEU: 26.28\n",
      "\n",
      "Epoch 40: Train Loss = 2.3006, Val Loss = 2.6568, \n",
      "Learning Rate: 0.000366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/120: 100%|██████████| 364/364 [01:22<00:00,  4.39it/s, loss=1.7048]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41: Train Loss = 2.2826, Val Loss = 2.6634, \n",
      "Learning Rate: 0.000362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/120: 100%|██████████| 364/364 [01:22<00:00,  4.39it/s, loss=1.7202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42: Train Loss = 2.2590, Val Loss = 2.6665, \n",
      "Learning Rate: 0.000357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/120: 100%|██████████| 364/364 [01:22<00:00,  4.39it/s, loss=1.6701]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43: Train Loss = 2.2448, Val Loss = 2.6692, \n",
      "Learning Rate: 0.000353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/120: 100%|██████████| 364/364 [01:22<00:00,  4.39it/s, loss=1.7505]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44: Train Loss = 2.2251, Val Loss = 2.6541, \n",
      "Learning Rate: 0.000349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/120: 100%|██████████| 364/364 [01:22<00:00,  4.39it/s, loss=1.6498]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45: Train Loss = 2.2082, Val Loss = 2.6700, \n",
      "Learning Rate: 0.000345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/120: 100%|██████████| 364/364 [01:22<00:00,  4.39it/s, loss=1.7087]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46: Train Loss = 2.1895, Val Loss = 2.6663, \n",
      "Learning Rate: 0.000342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/120: 100%|██████████| 364/364 [01:22<00:00,  4.39it/s, loss=1.6858]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47: Train Loss = 2.1781, Val Loss = 2.6684, \n",
      "Learning Rate: 0.000338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/120: 100%|██████████| 364/364 [01:23<00:00,  4.39it/s, loss=1.6742]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48: Train Loss = 2.1588, Val Loss = 2.6686, \n",
      "Learning Rate: 0.000334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/120: 100%|██████████| 364/364 [01:22<00:00,  4.39it/s, loss=1.6471]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49: Train Loss = 2.1438, Val Loss = 2.6726, \n",
      "Learning Rate: 0.000331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/120: 100%|██████████| 364/364 [01:22<00:00,  4.39it/s, loss=1.6699]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50: Train Loss = 2.1313, Val Loss = 2.6790, \n",
      "Learning Rate: 0.000328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/120: 100%|██████████| 364/364 [01:23<00:00,  4.38it/s, loss=1.6693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 51: Train Loss = 2.1179, Val Loss = 2.6813, \n",
      "Learning Rate: 0.000324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/120: 100%|██████████| 364/364 [01:22<00:00,  4.39it/s, loss=1.6570]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 52: Train Loss = 2.1045, Val Loss = 2.6828, \n",
      "Learning Rate: 0.000321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/120: 100%|██████████| 364/364 [01:22<00:00,  4.39it/s, loss=1.6260]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 53: Train Loss = 2.0911, Val Loss = 2.6905, \n",
      "Learning Rate: 0.000318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/120: 100%|██████████| 364/364 [01:22<00:00,  4.39it/s, loss=1.6721]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 54: Train Loss = 2.0789, Val Loss = 2.6810, \n",
      "Learning Rate: 0.000315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/120: 100%|██████████| 364/364 [01:22<00:00,  4.39it/s, loss=1.6675]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 55: Train Loss = 2.0622, Val Loss = 2.6803, \n",
      "Learning Rate: 0.000312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/120: 100%|██████████| 364/364 [01:22<00:00,  4.39it/s, loss=1.6110]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 56: Train Loss = 2.0498, Val Loss = 2.6840, \n",
      "Learning Rate: 0.000310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/120: 100%|██████████| 364/364 [01:22<00:00,  4.39it/s, loss=1.6560]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 57: Train Loss = 2.0409, Val Loss = 2.6923, \n",
      "Learning Rate: 0.000307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/120: 100%|██████████| 364/364 [01:22<00:00,  4.39it/s, loss=1.5704]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 58: Train Loss = 2.0314, Val Loss = 2.6944, \n",
      "Learning Rate: 0.000304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/120: 100%|██████████| 364/364 [01:23<00:00,  4.38it/s, loss=1.6071]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 59: Train Loss = 2.0196, Val Loss = 2.6996, \n",
      "Learning Rate: 0.000302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/120: 100%|██████████| 364/364 [01:22<00:00,  4.39it/s, loss=1.6463]\n",
      "Evaluating BLEU: 100%|██████████| 49/49 [07:09<00:00,  8.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 60] BLEU: 27.45\n",
      "🔥 Model terbaik disimpan dengan BLEU: 27.45\n",
      "\n",
      "Epoch 60: Train Loss = 2.0084, Val Loss = 2.6996, \n",
      "Learning Rate: 0.000299\n",
      "✅ Model disimpan: 23Maret-NoKeyMask-embffn5122048Layer6H4_model_epoch_60.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/120: 100%|██████████| 364/364 [01:23<00:00,  4.38it/s, loss=1.6573]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 61: Train Loss = 1.9984, Val Loss = 2.6937, \n",
      "Learning Rate: 0.000297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/120: 100%|██████████| 364/364 [01:22<00:00,  4.39it/s, loss=1.6134]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 62: Train Loss = 1.9891, Val Loss = 2.7103, \n",
      "Learning Rate: 0.000294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/120: 100%|██████████| 364/364 [01:23<00:00,  4.38it/s, loss=1.6101]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 63: Train Loss = 1.9792, Val Loss = 2.7171, \n",
      "Learning Rate: 0.000292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/120: 100%|██████████| 364/364 [01:23<00:00,  4.38it/s, loss=1.6007]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 64: Train Loss = 1.9729, Val Loss = 2.7226, \n",
      "Learning Rate: 0.000290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/120: 100%|██████████| 364/364 [01:22<00:00,  4.39it/s, loss=1.5698]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 65: Train Loss = 1.9635, Val Loss = 2.7038, \n",
      "Learning Rate: 0.000287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/120: 100%|██████████| 364/364 [01:22<00:00,  4.39it/s, loss=1.5863]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 66: Train Loss = 1.9584, Val Loss = 2.7093, \n",
      "Learning Rate: 0.000285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/120: 100%|██████████| 364/364 [01:23<00:00,  4.38it/s, loss=1.5839]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 67: Train Loss = 1.9466, Val Loss = 2.7086, \n",
      "Learning Rate: 0.000283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68/120: 100%|██████████| 364/364 [01:22<00:00,  4.39it/s, loss=1.5911]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 68: Train Loss = 1.9380, Val Loss = 2.7167, \n",
      "Learning Rate: 0.000281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/120: 100%|██████████| 364/364 [01:23<00:00,  4.39it/s, loss=1.6304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 69: Train Loss = 1.9302, Val Loss = 2.7143, \n",
      "Learning Rate: 0.000279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70/120: 100%|██████████| 364/364 [01:23<00:00,  4.37it/s, loss=1.5805]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 70: Train Loss = 1.9216, Val Loss = 2.7122, \n",
      "Learning Rate: 0.000277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71/120: 100%|██████████| 364/364 [01:23<00:00,  4.35it/s, loss=1.5947]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 71: Train Loss = 1.9137, Val Loss = 2.7133, \n",
      "Learning Rate: 0.000275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72/120: 100%|██████████| 364/364 [01:23<00:00,  4.35it/s, loss=1.5832]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 72: Train Loss = 1.9073, Val Loss = 2.7282, \n",
      "Learning Rate: 0.000273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73/120: 100%|██████████| 364/364 [01:23<00:00,  4.35it/s, loss=1.6244]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 73: Train Loss = 1.9002, Val Loss = 2.7166, \n",
      "Learning Rate: 0.000271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74/120: 100%|██████████| 364/364 [01:23<00:00,  4.35it/s, loss=1.5993]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 74: Train Loss = 1.8933, Val Loss = 2.7051, \n",
      "Learning Rate: 0.000269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75/120: 100%|██████████| 364/364 [01:23<00:00,  4.36it/s, loss=1.5824]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 75: Train Loss = 1.8852, Val Loss = 2.7251, \n",
      "Learning Rate: 0.000267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76/120: 100%|██████████| 364/364 [01:23<00:00,  4.36it/s, loss=1.6053]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 76: Train Loss = 1.8793, Val Loss = 2.7258, \n",
      "Learning Rate: 0.000266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77/120: 100%|██████████| 364/364 [01:23<00:00,  4.35it/s, loss=1.5757]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 77: Train Loss = 1.8744, Val Loss = 2.7368, \n",
      "Learning Rate: 0.000264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78/120: 100%|██████████| 364/364 [01:23<00:00,  4.35it/s, loss=1.6320]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 78: Train Loss = 1.8655, Val Loss = 2.7194, \n",
      "Learning Rate: 0.000262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79/120: 100%|██████████| 364/364 [01:23<00:00,  4.36it/s, loss=1.5765]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 79: Train Loss = 1.8617, Val Loss = 2.7349, \n",
      "Learning Rate: 0.000261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80/120: 100%|██████████| 364/364 [01:23<00:00,  4.36it/s, loss=1.5302]\n",
      "Evaluating BLEU: 100%|██████████| 49/49 [08:11<00:00, 10.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 80] BLEU: 28.27\n",
      "🔥 Model terbaik disimpan dengan BLEU: 28.27\n",
      "\n",
      "Epoch 80: Train Loss = 1.8538, Val Loss = 2.7202, \n",
      "Learning Rate: 0.000259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81/120: 100%|██████████| 364/364 [01:23<00:00,  4.34it/s, loss=1.5542]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 81: Train Loss = 1.8482, Val Loss = 2.7334, \n",
      "Learning Rate: 0.000257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82/120: 100%|██████████| 364/364 [01:23<00:00,  4.35it/s, loss=1.5516]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 82: Train Loss = 1.8468, Val Loss = 2.7086, \n",
      "Learning Rate: 0.000256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83/120: 100%|██████████| 364/364 [01:23<00:00,  4.34it/s, loss=1.5426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 83: Train Loss = 1.8409, Val Loss = 2.7221, \n",
      "Learning Rate: 0.000254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84/120: 100%|██████████| 364/364 [01:23<00:00,  4.34it/s, loss=1.5891]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 84: Train Loss = 1.8319, Val Loss = 2.7335, \n",
      "Learning Rate: 0.000253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85/120: 100%|██████████| 364/364 [01:23<00:00,  4.35it/s, loss=1.5566]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 85: Train Loss = 1.8283, Val Loss = 2.7342, \n",
      "Learning Rate: 0.000251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86/120: 100%|██████████| 364/364 [01:23<00:00,  4.35it/s, loss=1.5271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 86: Train Loss = 1.8237, Val Loss = 2.7247, \n",
      "Learning Rate: 0.000250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87/120: 100%|██████████| 364/364 [01:23<00:00,  4.35it/s, loss=1.5790]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 87: Train Loss = 1.8169, Val Loss = 2.7347, \n",
      "Learning Rate: 0.000248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88/120: 100%|██████████| 364/364 [01:23<00:00,  4.35it/s, loss=1.5394]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 88: Train Loss = 1.8137, Val Loss = 2.7269, \n",
      "Learning Rate: 0.000247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89/120: 100%|██████████| 364/364 [01:23<00:00,  4.34it/s, loss=1.5338]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 89: Train Loss = 1.8065, Val Loss = 2.7358, \n",
      "Learning Rate: 0.000246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90/120: 100%|██████████| 364/364 [01:23<00:00,  4.34it/s, loss=1.5480]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 90: Train Loss = 1.8020, Val Loss = 2.7437, \n",
      "Learning Rate: 0.000244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91/120: 100%|██████████| 364/364 [01:23<00:00,  4.35it/s, loss=1.5775]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 91: Train Loss = 1.7989, Val Loss = 2.7325, \n",
      "Learning Rate: 0.000243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92/120: 100%|██████████| 364/364 [01:23<00:00,  4.35it/s, loss=1.5243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 92: Train Loss = 1.7929, Val Loss = 2.7417, \n",
      "Learning Rate: 0.000242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93/120: 100%|██████████| 364/364 [01:23<00:00,  4.35it/s, loss=1.5243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 93: Train Loss = 1.7887, Val Loss = 2.7487, \n",
      "Learning Rate: 0.000240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94/120: 100%|██████████| 364/364 [01:23<00:00,  4.34it/s, loss=1.5411]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 94: Train Loss = 1.7830, Val Loss = 2.7333, \n",
      "Learning Rate: 0.000239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95/120: 100%|██████████| 364/364 [01:23<00:00,  4.35it/s, loss=1.5665]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 95: Train Loss = 1.7829, Val Loss = 2.7370, \n",
      "Learning Rate: 0.000238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96/120: 100%|██████████| 364/364 [01:23<00:00,  4.35it/s, loss=1.5894]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 96: Train Loss = 1.7791, Val Loss = 2.7824, \n",
      "Learning Rate: 0.000236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97/120: 100%|██████████| 364/364 [01:23<00:00,  4.35it/s, loss=1.5239]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 97: Train Loss = 1.7732, Val Loss = 2.7522, \n",
      "Learning Rate: 0.000235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98/120: 100%|██████████| 364/364 [01:23<00:00,  4.35it/s, loss=1.5478]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 98: Train Loss = 1.7671, Val Loss = 2.7432, \n",
      "Learning Rate: 0.000234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99/120: 100%|██████████| 364/364 [01:23<00:00,  4.35it/s, loss=1.5726]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 99: Train Loss = 1.7637, Val Loss = 2.7521, \n",
      "Learning Rate: 0.000233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100/120: 100%|██████████| 364/364 [01:23<00:00,  4.35it/s, loss=1.5475]\n",
      "Evaluating BLEU: 100%|██████████| 49/49 [08:09<00:00, 10.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 100] BLEU: 28.85\n",
      "🔥 Model terbaik disimpan dengan BLEU: 28.85\n",
      "\n",
      "Epoch 100: Train Loss = 1.7611, Val Loss = 2.7416, \n",
      "Learning Rate: 0.000232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 101/120: 100%|██████████| 364/364 [01:23<00:00,  4.36it/s, loss=1.5224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 101: Train Loss = 1.7567, Val Loss = 2.7596, \n",
      "Learning Rate: 0.000230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 102/120: 100%|██████████| 364/364 [01:23<00:00,  4.35it/s, loss=1.5276]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 102: Train Loss = 1.7509, Val Loss = 2.7506, \n",
      "Learning Rate: 0.000229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 103/120: 100%|██████████| 364/364 [01:23<00:00,  4.35it/s, loss=1.5423]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 103: Train Loss = 1.7555, Val Loss = 2.7531, \n",
      "Learning Rate: 0.000228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 104/120: 100%|██████████| 364/364 [01:23<00:00,  4.35it/s, loss=1.5505]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 104: Train Loss = 1.7464, Val Loss = 2.7494, \n",
      "Learning Rate: 0.000227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 105/120: 100%|██████████| 364/364 [01:23<00:00,  4.36it/s, loss=1.5662]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 105: Train Loss = 1.7419, Val Loss = 2.7541, \n",
      "Learning Rate: 0.000226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 106/120: 100%|██████████| 364/364 [01:23<00:00,  4.36it/s, loss=1.5562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 106: Train Loss = 1.7395, Val Loss = 2.7451, \n",
      "Learning Rate: 0.000225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 107/120: 100%|██████████| 364/364 [01:23<00:00,  4.36it/s, loss=1.5379]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 107: Train Loss = 1.7351, Val Loss = 2.7606, \n",
      "Learning Rate: 0.000224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 108/120: 100%|██████████| 364/364 [01:23<00:00,  4.38it/s, loss=1.5086]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 108: Train Loss = 1.7321, Val Loss = 2.7558, \n",
      "Learning Rate: 0.000223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 109/120: 100%|██████████| 364/364 [01:23<00:00,  4.38it/s, loss=1.5745]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 109: Train Loss = 1.7281, Val Loss = 2.7489, \n",
      "Learning Rate: 0.000222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 110/120: 100%|██████████| 364/364 [01:23<00:00,  4.38it/s, loss=1.5079]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 110: Train Loss = 1.7238, Val Loss = 2.7496, \n",
      "Learning Rate: 0.000221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 111/120: 100%|██████████| 364/364 [01:23<00:00,  4.38it/s, loss=1.5015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 111: Train Loss = 1.7225, Val Loss = 2.7609, \n",
      "Learning Rate: 0.000220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 112/120: 100%|██████████| 364/364 [01:23<00:00,  4.38it/s, loss=1.5458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 112: Train Loss = 1.7208, Val Loss = 2.7560, \n",
      "Learning Rate: 0.000219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 113/120: 100%|██████████| 364/364 [01:23<00:00,  4.37it/s, loss=1.5337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 113: Train Loss = 1.7160, Val Loss = 2.7647, \n",
      "Learning Rate: 0.000218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 114/120: 100%|██████████| 364/364 [01:23<00:00,  4.38it/s, loss=1.5336]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 114: Train Loss = 1.7141, Val Loss = 2.7602, \n",
      "Learning Rate: 0.000217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 115/120: 100%|██████████| 364/364 [01:23<00:00,  4.37it/s, loss=1.5179]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 115: Train Loss = 1.7107, Val Loss = 2.7719, \n",
      "Learning Rate: 0.000216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 116/120: 100%|██████████| 364/364 [01:23<00:00,  4.38it/s, loss=1.5279]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 116: Train Loss = 1.7088, Val Loss = 2.7604, \n",
      "Learning Rate: 0.000215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 117/120: 100%|██████████| 364/364 [01:23<00:00,  4.38it/s, loss=1.5015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 117: Train Loss = 1.7043, Val Loss = 2.7631, \n",
      "Learning Rate: 0.000214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 118/120: 100%|██████████| 364/364 [01:23<00:00,  4.38it/s, loss=1.5289]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 118: Train Loss = 1.7013, Val Loss = 2.7754, \n",
      "Learning Rate: 0.000213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 119/120: 100%|██████████| 364/364 [01:23<00:00,  4.38it/s, loss=1.5083]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 119: Train Loss = 1.6979, Val Loss = 2.7693, \n",
      "Learning Rate: 0.000212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 120/120: 100%|██████████| 364/364 [01:23<00:00,  4.38it/s, loss=1.5387]\n",
      "Evaluating BLEU: 100%|██████████| 49/49 [07:02<00:00,  8.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 120] BLEU: 29.26\n",
      "🔥 Model terbaik disimpan dengan BLEU: 29.26\n",
      "\n",
      "Epoch 120: Train Loss = 1.6977, Val Loss = 2.7651, \n",
      "Learning Rate: 0.000211\n",
      "✅ Model disimpan: 23Maret-NoKeyMask-embffn5122048Layer6H4_model_epoch_120.pth\n"
     ]
    }
   ],
   "source": [
    "history = train(model, dataloader, val_dataloader, optimizer, scheduler=scheduler, num_epochs=120, device=device, src_vocab=src_vocab, tgt_vocab=tgt_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e772a6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T23:42:31.771090Z",
     "iopub.status.busy": "2025-03-27T23:42:31.770758Z",
     "iopub.status.idle": "2025-03-27T23:42:31.775013Z",
     "shell.execute_reply": "2025-03-27T23:42:31.774321Z"
    },
    "papermill": {
     "duration": 4.379192,
     "end_time": "2025-03-27T23:42:31.776119",
     "exception": false,
     "start_time": "2025-03-27T23:42:27.396927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Specify the file path where you want to save the data\n",
    "file_path = f'Run-{DATE}.pkl'\n",
    "\n",
    "with open(file_path, 'wb') as file:\n",
    "    pickle.dump(history, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0716db60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T23:42:40.622872Z",
     "iopub.status.busy": "2025-03-27T23:42:40.622546Z",
     "iopub.status.idle": "2025-03-27T23:42:40.632744Z",
     "shell.execute_reply": "2025-03-27T23:42:40.631898Z"
    },
    "papermill": {
     "duration": 4.474757,
     "end_time": "2025-03-27T23:42:40.634084",
     "exception": false,
     "start_time": "2025-03-27T23:42:36.159327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def beam_search_dataloader(model, dataloader, src_vocab, tgt_vocab, device, beam_size=5, max_len=64, temperature=1.0, output_file=\"beam_translations.txt\"):\n",
    "    model.eval()\n",
    "    translations = []\n",
    "    references = []\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for src, tgt in tqdm(dataloader, desc=\"Beam Search Decoding\"):\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            batch_translations = []\n",
    "\n",
    "            for i in range(src.size(0)):\n",
    "                src_i = src[i].unsqueeze(0)  # Ambil satu contoh\n",
    "                enc_output = model.encoder(model.dropout(model.pos_encoding(model.src_embedding(src_i)))).to(device)\n",
    "\n",
    "                # Inisialisasi beam search dengan (score, token sequence)\n",
    "                beams = [(0, [tgt_vocab[\"<sos>\"]])]\n",
    "\n",
    "                for _ in range(max_len):\n",
    "                    all_candidates = []\n",
    "                    for score, tgt_tokens in beams:\n",
    "                        if tgt_tokens[-1] == tgt_vocab[\"<eos>\"]:\n",
    "                            all_candidates.append((score, tgt_tokens))\n",
    "                            continue\n",
    "\n",
    "                        tgt_tensor = torch.tensor(tgt_tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
    "                        tgt_emb = model.dropout(model.pos_encoding(model.tgt_embedding(tgt_tensor)))\n",
    "                        tgt_mask = model.generate_subsequent_mask(tgt_tensor.size(1)).to(device)\n",
    "\n",
    "                        dec_output = model.decoder(tgt_emb, enc_output, tgt_mask=tgt_mask)\n",
    "                        logits = model.fc_out(dec_output[:, -1, :])  # Tetap di GPU\n",
    "\n",
    "                        logits = logits / temperature\n",
    "                        probs = F.softmax(logits, dim=-1)\n",
    "                        log_probs = torch.log(probs + 1e-9)\n",
    "\n",
    "                        top_log_probs, top_indices = log_probs.topk(beam_size)\n",
    "                        for j in range(beam_size):\n",
    "                            next_token = top_indices[0, j].item()\n",
    "                            new_score = score + top_log_probs[0, j].item()\n",
    "                            new_sequence = tgt_tokens + [next_token]\n",
    "                            all_candidates.append((new_score, new_sequence))\n",
    "\n",
    "                    beams = sorted(all_candidates, key=lambda x: x[0], reverse=True)[:beam_size]\n",
    "                    if all(seq[-1] == tgt_vocab[\"<eos>\"] for _, seq in beams):\n",
    "                        break\n",
    "\n",
    "                best_sequence = max(beams, key=lambda x: x[0])[1]\n",
    "                best_sequence = [tok for tok in best_sequence if tok != tgt_vocab[\"<sos>\"]]  # Hapus <sos>\n",
    "                translation = tensor_to_text(torch.tensor(best_sequence, dtype=torch.long), tgt_vocab)\n",
    "                batch_translations.append(translation)\n",
    "\n",
    "            translations.extend(batch_translations)\n",
    "            references.extend([tensor_to_text(t, tgt_vocab) for t in tgt])\n",
    "\n",
    "            for trans in batch_translations:\n",
    "                f.write(trans + \"\\n\")\n",
    "\n",
    "    print(f\"Terjemahan berhasil disimpan di '{output_file}'\")\n",
    "    return translations, references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2669b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f\"models/{DATE}_best_model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c214f8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T23:42:49.348564Z",
     "iopub.status.busy": "2025-03-27T23:42:49.348226Z",
     "iopub.status.idle": "2025-03-28T00:03:38.344805Z",
     "shell.execute_reply": "2025-03-28T00:03:38.343802Z"
    },
    "papermill": {
     "duration": 1253.330917,
     "end_time": "2025-03-28T00:03:38.346121",
     "exception": false,
     "start_time": "2025-03-27T23:42:45.015204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beam Search Decoding: 100%|██████████| 49/49 [20:48<00:00, 25.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terjemahan berhasil disimpan di '23Maret-NoKeyMask-embffn5122048Layer6H4_beam_translations.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "translations = beam_search_dataloader(\n",
    "    model, val_dataloader, src_vocab, tgt_vocab, device, beam_size=3, temperature=0.7, output_file=f\"{DATE}_beam_translations.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eab81081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beam Search Decoding:   0%|          | 0/73 [00:00<?, ?it/s]c:\\Users\\SC3\\AppData\\Local\\anaconda3\\envs\\yeo\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:720: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  return torch._transformer_encoder_layer_fwd(\n",
      "Beam Search Decoding: 100%|██████████| 73/73 [1:05:50<00:00, 54.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terjemahan berhasil disimpan di '23Maret-NoKeyMask-embffn5122048Layer6H4_beam_translations-test.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "translations = beam_search_dataloader(\n",
    "    model, test_dataloader, src_vocab, tgt_vocab, device, beam_size=3, temperature=0.7, output_file=f\"{DATE}_beam_translations-test.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "895e0cbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T00:04:04.737901Z",
     "iopub.status.busy": "2025-03-28T00:04:04.737569Z",
     "iopub.status.idle": "2025-03-28T00:04:05.095097Z",
     "shell.execute_reply": "2025-03-28T00:04:05.094242Z"
    },
    "papermill": {
     "duration": 4.743973,
     "end_time": "2025-03-28T00:04:05.096445",
     "exception": false,
     "start_time": "2025-03-28T00:04:00.352472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.24583568309559\n"
     ]
    }
   ],
   "source": [
    "hypothesis = translations[0]\n",
    "reference = [[text] for text in translations[1]]\n",
    "\n",
    "print(sacrebleu.corpus_bleu(hypothesis, reference).score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c422c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.33172938363535\n"
     ]
    }
   ],
   "source": [
    "hypothesis = translations[0]\n",
    "reference = [[text] for text in translations[1]]\n",
    "\n",
    "print(sacrebleu.corpus_bleu(hypothesis, reference).score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ec20c9ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T00:03:47.270678Z",
     "iopub.status.busy": "2025-03-28T00:03:47.270325Z",
     "iopub.status.idle": "2025-03-28T00:03:47.274552Z",
     "shell.execute_reply": "2025-03-28T00:03:47.273810Z"
    },
    "papermill": {
     "duration": 4.512816,
     "end_time": "2025-03-28T00:03:47.275822",
     "exception": false,
     "start_time": "2025-03-28T00:03:42.763006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bleu_by_sentence(hypothesis, references):\n",
    "    \"\"\"\n",
    "    Menghitung skor BLEU untuk satu hipotesis terhadap satu atau lebih referensi menggunakan sacrebleu.\n",
    "    \n",
    "    Parameters:\n",
    "        hypothesis (str): Kalimat hipotesis.\n",
    "        references (list of str): Daftar kalimat referensi.\n",
    "    \n",
    "    Returns:\n",
    "        float: Skor BLEU\n",
    "    \"\"\"\n",
    "    bleu_score = sacrebleu.sentence_bleu(hypothesis, references).score\n",
    "    return bleu_score\n",
    "\n",
    "# Contoh penggunaan\n",
    "hypothesis = \"the cat is on the mat\"\n",
    "reference = [\"there is a cat on the mat\", \"the cat sits on the mat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c631be92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T00:04:48.867075Z",
     "iopub.status.busy": "2025-03-28T00:04:48.866670Z",
     "iopub.status.idle": "2025-03-28T00:04:48.873963Z",
     "shell.execute_reply": "2025-03-28T00:04:48.873179Z"
    },
    "papermill": {
     "duration": 4.452075,
     "end_time": "2025-03-28T00:04:48.875163",
     "exception": false,
     "start_time": "2025-03-28T00:04:44.423088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerMT(\n",
       "  (src_embedding): Embedding(23876, 512)\n",
       "  (tgt_embedding): Embedding(15229, 512)\n",
       "  (pos_encoding): PositionalEncoding()\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.3, inplace=False)\n",
       "        (dropout2): Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.3, inplace=False)\n",
       "        (dropout2): Dropout(p=0.3, inplace=False)\n",
       "        (dropout3): Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (fc_out): Linear(in_features=512, out_features=15229, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6860018,
     "sourceId": 11156153,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "yeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14664.059184,
   "end_time": "2025-03-28T00:04:54.871983",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-27T20:00:30.812799",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
